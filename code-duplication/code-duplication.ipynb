{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Code Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark.environment import IMPROVEMENTS, STATES\n",
    "from benchmark.process import run\n",
    "from requests import get, post\n",
    "from os import chdir\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import docker\n",
    "import git\n",
    "\n",
    "\n",
    "SONAR_PORT = 9999\n",
    "SONAR_PASSWORD = \"password\"\n",
    "\n",
    "\n",
    "if not \"PROJECT_DIR\" in globals():\n",
    "    PROJECT_DIR = Path().resolve().parent\n",
    "    chdir(PROJECT_DIR)\n",
    "\n",
    "\n",
    "if not \"OUT_DIR\" in globals():\n",
    "    OUT_DIR = PROJECT_DIR / \"code-duplication\" / \"out\"\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "if not \"REPO_DIR\" in globals():\n",
    "    REPO_DIR = PROJECT_DIR / \"fda-services\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyze the code quality of DBRepo, we make use of SonarQube running in a Docker container. The following command can be used to start an instance of SonarQube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_client = docker.from_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container: 74e8a5c9e831>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docker_client.containers.run(\n",
    "    \"sonarqube@sha256:72e9feec71242af83faf65f95a40d5e3bb2822a6c3b2cda8568790f3d31aecde\",\n",
    "    detach=True,\n",
    "    name=\"sonarqube\",\n",
    "    environment={\"SONAR_ES_BOOTSTRAP_CHECKS_DISABLE\": \"true\"},\n",
    "    ports={\"9000\": SONAR_PORT},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze DBRepo's code, we clone the repository and initialize an instance of the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REPO_DIR.exists():\n",
    "    git.Git(PROJECT_DIR).clone(\n",
    "        \"https://gitlab.phaidra.org/fair-data-austria-db-repository/fda-services.git\"\n",
    "    )\n",
    "\n",
    "repo = git.Repo(REPO_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the container has successfully started, the repo is cloned and the web interface is accessible at [http://localhost:9999](), we first have to change the password of the default admin user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = post(\n",
    "    f\"http://localhost:{SONAR_PORT}/api/users/change_password\",\n",
    "    auth=(\"admin\", \"admin\"),\n",
    "    data={\"login\": \"admin\", \"password\": SONAR_PASSWORD, \"previousPassword\": \"admin\"},\n",
    ")\n",
    "response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we generate a token for the admin user to be able to analyze the code quality of DBRepo and store it in an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = post(\n",
    "    f\"http://localhost:{SONAR_PORT}/api/user_tokens/generate\",\n",
    "    auth=(\"admin\", SONAR_PASSWORD),\n",
    "    data={\n",
    "        \"login\": \"admin\",\n",
    "        \"name\": \"Global Analysis Token\",\n",
    "        \"type\": \"GLOBAL_ANALYSIS_TOKEN\",\n",
    "    },\n",
    ")\n",
    "response.raise_for_status()\n",
    "sonar_token = response.json()[\"token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a branch of the state of the repository before and after each improvement based on the commit hashes specified in [STATES.md](../STATES.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.git.branch(\n",
    "    \"environment-independence_before\", \"8bb23619997f1d6f85d85718eb5eb018f68bd80f\"\n",
    ")\n",
    "repo.git.branch(\n",
    "    \"environment-independence_after\", \"7cf0c76094c285a02ad3341685969733d6836164\"\n",
    ")\n",
    "repo.git.branch(\"service-merge_before\", \"82cd375098246e38cf1da9ad34ee981a637433b7\")\n",
    "repo.git.branch(\"service-merge_after\", \"683d2096bf20fe2c9703aab181c932e17550c3e7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to start the analysis. The following script can be used to analyze the code quality of DBRepo before and after each improvement. We create a project for each improvement and state because of limitations of the SonarQube community edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for improvement in IMPROVEMENTS:\n",
    "    for state in STATES:\n",
    "        key = f\"{improvement}_{state}\"\n",
    "\n",
    "        response = post(\n",
    "            f\"http://localhost:{SONAR_PORT}/api/projects/create\",\n",
    "            auth=(\"admin\", SONAR_PASSWORD),\n",
    "            data={\n",
    "                \"creationMode\": \"manual\",\n",
    "                \"monorepo\": \"false\",\n",
    "                \"project\": key,\n",
    "                \"name\": key,\n",
    "                \"mainBranch\": \"main\",\n",
    "            },\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        repo.git.checkout(key)\n",
    "\n",
    "        run(\"rm -rf ./*/*/target/\", cwd=REPO_DIR, shell=True)\n",
    "        # run('make build-backend', cwd=REPO_DIR, shell=True)\n",
    "        run(\n",
    "            'eval \"$(pyenv init -)\" && make build-backend', cwd=REPO_DIR, shell=True\n",
    "        )  # activating pyenv is necessary in my environment, remove if not needed in yours\n",
    "\n",
    "        docker_client.containers.run(\n",
    "            \"sonarsource/sonar-scanner-cli@sha256:605ea9a44a12ec328ad59a9b98c740cf0672a467b57ba2ae63cb85cc5831287e\",\n",
    "            remove=True,\n",
    "            network_mode=\"host\",\n",
    "            volumes={REPO_DIR: {\"bind\": \"/usr/src\"}},\n",
    "            command=[\n",
    "                \"-Dsonar.projectKey=\" + key,\n",
    "                \"-Dsonar.projectName=\" + key,\n",
    "                \"-Dsonar.host.url=http://localhost:\" + str(SONAR_PORT),\n",
    "                \"-Dsonar.token=\" + sonar_token,\n",
    "                \"-Dsonar.java.binaries=\"\n",
    "                + \",\".join(\n",
    "                    [\n",
    "                        str(path.relative_to(REPO_DIR))\n",
    "                        for path in REPO_DIR.glob(\"*/*/target/classes/\")\n",
    "                    ]\n",
    "                ),\n",
    "                \"-Dsonar.java.libraries=**/*.jar\",\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the analysis has finished, we can extract information about the code replication from the projects and store them in CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_data = []\n",
    "\n",
    "for improvement in IMPROVEMENTS:\n",
    "    for state in STATES:\n",
    "        key = f\"{improvement}_{state}\"\n",
    "\n",
    "        response = get(\n",
    "            f\"http://localhost:{SONAR_PORT}/api/measures/component_tree\",\n",
    "            auth=(\"admin\", SONAR_PASSWORD),\n",
    "            params={\n",
    "                \"component\": key,\n",
    "                \"s\": \"qualifier,name\",\n",
    "                \"metricKeys\": \"ncloc,duplicated_lines,duplicated_lines_density\",\n",
    "                \"strategy\": \"children\",\n",
    "            },\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        json = response.json()\n",
    "\n",
    "        overall_metrics = {\n",
    "            measure[\"metric\"]: measure[\"value\"]\n",
    "            for measure in json[\"baseComponent\"][\"measures\"]\n",
    "        }\n",
    "        overall_data.append(\n",
    "            {\n",
    "                \"improvement\": improvement,\n",
    "                \"state\": state,\n",
    "                \"lines\": overall_metrics[\"ncloc\"],\n",
    "                \"duplicated\": overall_metrics[\"duplicated_lines\"],\n",
    "                \"duplicated-density\": overall_metrics[\"duplicated_lines_density\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        services = []\n",
    "\n",
    "        for component in json[\"components\"]:\n",
    "            if component[\"qualifier\"] != \"DIR\":\n",
    "                print(component[\"qualifier\"])\n",
    "                continue\n",
    "\n",
    "            service = {\"service\": component[\"name\"]}\n",
    "            service.update(\n",
    "                {\n",
    "                    measure[\"metric\"]: measure[\"value\"]\n",
    "                    for measure in component[\"measures\"]\n",
    "                }\n",
    "            )\n",
    "            services.append(service)\n",
    "\n",
    "        service_dataframe = pd.DataFrame(services)\n",
    "        service_dataframe[\"improvement\"] = improvement\n",
    "        service_dataframe[\"state\"] = state\n",
    "        service_dataframe.rename(\n",
    "            columns={\n",
    "                \"name\": \"service\",\n",
    "                \"ncloc\": \"lines\",\n",
    "                \"duplicated_lines\": \"duplicated\",\n",
    "                \"duplicated_lines_density\": \"duplicated-density\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        service_dataframe = service_dataframe[\n",
    "            [\n",
    "                \"improvement\",\n",
    "                \"state\",\n",
    "                \"service\",\n",
    "                \"lines\",\n",
    "                \"duplicated\",\n",
    "                \"duplicated-density\",\n",
    "            ]\n",
    "        ]\n",
    "        service_dataframe.to_csv(\n",
    "            OUT_DIR / f\"service-duplication_{improvement}_{state}.csv\", index=False\n",
    "        )\n",
    "\n",
    "overall = pd.DataFrame(overall_data)\n",
    "overall = overall[[\"improvement\", \"state\", \"lines\", \"duplicated\", \"duplicated-density\"]]\n",
    "overall.to_csv(OUT_DIR / \"overall-duplication.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can export the results of the code replication analysis previously stored in CSV files as latex tables, to include them in the bachelor's thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrr}\n",
      "\\toprule\n",
      " &  & Lines & Duplicated & Duplicated Density \\\\\n",
      "Improvement & State &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[t]{2}{*}{Environment Independence} & Before & 60417 & 3977 & 5.300000 \\\\\n",
      " & After & 59232 & 4300 & 5.900000 \\\\\n",
      "\\cline{1-5}\n",
      "\\multirow[t]{2}{*}{Service Merge} & Before & 71913 & 6135 & 7.000000 \\\\\n",
      " & After & 57515 & 2245 & 3.200000 \\\\\n",
      "\\cline{1-5}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(OUT_DIR / \"overall-duplication.csv\")\n",
    "df.columns = df.columns.str.replace(\"-\", \" \").str.title()\n",
    "df[\"Improvement\"] = df[\"Improvement\"].str.replace(\"-\", \" \").str.title()\n",
    "df[\"State\"] = df[\"State\"].str.title()\n",
    "df.set_index([\"Improvement\", \"State\"], inplace=True)\n",
    "print(df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "Service & Lines & Duplicated & Duplicated Density \\\\\n",
      "\\midrule\n",
      "Analyse Service & 550 & 0 & 0.000000 \\\\\n",
      "Authentication Service & 30 & 0 & 0.000000 \\\\\n",
      "Broker Service & 14 & 0 & 0.000000 \\\\\n",
      "Container Service & 3710 & 269 & 5.600000 \\\\\n",
      "Database Service & 5810 & 504 & 6.600000 \\\\\n",
      "Identifier Service & 4517 & 389 & 6.700000 \\\\\n",
      "Metadata Database & 11212 & 604 & 3.900000 \\\\\n",
      "Metadata Service & 1513 & 178 & 9.600000 \\\\\n",
      "Query Service & 9527 & 872 & 7.200000 \\\\\n",
      "Semantics Service & 3279 & 299 & 7.300000 \\\\\n",
      "Table Service & 4530 & 547 & 9.500000 \\\\\n",
      "UI Service & 12343 & 141 & 1.100000 \\\\\n",
      "User Service & 3382 & 174 & 4.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "Service & Lines & Duplicated & Duplicated Density \\\\\n",
      "\\midrule\n",
      "Analyse Service & 550 & 0 & 0.000000 \\\\\n",
      "Authentication Service & 30 & 0 & 0.000000 \\\\\n",
      "Broker Service & 14 & 0 & 0.000000 \\\\\n",
      "Container Service & 2964 & 269 & 7.000000 \\\\\n",
      "Database Service & 5728 & 689 & 9.200000 \\\\\n",
      "Identifier Service & 4495 & 387 & 6.700000 \\\\\n",
      "Metadata Database & 11048 & 556 & 3.800000 \\\\\n",
      "Metadata Service & 1514 & 178 & 9.600000 \\\\\n",
      "Query Service & 9458 & 832 & 6.900000 \\\\\n",
      "Semantics Service & 3280 & 299 & 7.300000 \\\\\n",
      "Table Service & 4657 & 801 & 13.500000 \\\\\n",
      "UI Service & 12111 & 115 & 0.900000 \\\\\n",
      "User Service & 3383 & 174 & 4.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "Service & Lines & Duplicated & Duplicated Density \\\\\n",
      "\\midrule\n",
      "Analyse Service & 556 & 0 & 0.000000 \\\\\n",
      "Authentication Service & 33 & 0 & 0.000000 \\\\\n",
      "Broker Service & 14 & 0 & 0.000000 \\\\\n",
      "Container Service & 3069 & 299 & 7.600000 \\\\\n",
      "Data Service & 2288 & 355 & 12.300000 \\\\\n",
      "Database Service & 5780 & 685 & 9.100000 \\\\\n",
      "Identifier Service & 5596 & 660 & 9.200000 \\\\\n",
      "Metadata Database & 14686 & 425 & 2.300000 \\\\\n",
      "Metadata Service & 1769 & 277 & 12.500000 \\\\\n",
      "Query Service & 9834 & 772 & 6.200000 \\\\\n",
      "Search Sync Agent & 2137 & 336 & 12.400000 \\\\\n",
      "Semantics Service & 3237 & 301 & 7.500000 \\\\\n",
      "Table Service & 5119 & 779 & 12.000000 \\\\\n",
      "UI Service & 14439 & 1034 & 6.800000 \\\\\n",
      "User Service & 3356 & 212 & 5.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\\begin{tabular}{lrrr}\n",
      "\\toprule\n",
      "Service & Lines & Duplicated & Duplicated Density \\\\\n",
      "\\midrule\n",
      "Analyse Service & 556 & 0 & 0.000000 \\\\\n",
      "Authentication Service & 33 & 0 & 0.000000 \\\\\n",
      "Broker Service & 14 & 0 & 0.000000 \\\\\n",
      "Metadata Database & 9 & 0 & 0.000000 \\\\\n",
      "Metadata Service & 41345 & 1140 & 2.200000 \\\\\n",
      "Search Sync Agent & 1119 & 71 & 5.400000 \\\\\n",
      "UI Service & 14439 & 1034 & 6.800000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for improvement in IMPROVEMENTS:\n",
    "    for state in STATES:\n",
    "        df = pd.read_csv(OUT_DIR / f\"service-duplication_{improvement}_{state}.csv\")\n",
    "        df.columns = df.columns.str.replace(\"-\", \" \").str.title()\n",
    "        df = df.drop(columns=[\"Improvement\", \"State\"])\n",
    "        df[\"Service\"] = (\n",
    "            df[\"Service\"]\n",
    "            .str.replace(\"dbrepo-\", \"\")\n",
    "            .str.replace(\"-\", \" \")\n",
    "            .str.replace(\"db\", \"database\")\n",
    "            .str.title()\n",
    "            .replace(\"Ui\", \"UI Service\")\n",
    "        )\n",
    "        print(df.to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark-OW9XeZDE-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
